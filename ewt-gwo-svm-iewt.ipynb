{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b0921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0d3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewt_boundaries_equal_energy(spectrum, N):\n",
    "    energy = np.cumsum(spectrum) / np.sum(spectrum)\n",
    "    boundaries = []\n",
    "    for k in range(1, N):\n",
    "        idx = np.argmin(np.abs(energy - k / N))\n",
    "        boundaries.append(idx / len(spectrum) * np.pi)\n",
    "    return boundaries\n",
    "\n",
    "def ewt_boundaries(spectrum, N, smooth_sigma=2):\n",
    "    spectrum_smooth = gaussian_filter1d(spectrum, sigma=smooth_sigma)\n",
    "    if np.allclose(spectrum_smooth, spectrum_smooth[0]):\n",
    "        return ewt_boundaries_equal_energy(spectrum, N)\n",
    "    peaks, _ = find_peaks(spectrum_smooth)\n",
    "    if len(peaks) < N - 1:\n",
    "        return ewt_boundaries_equal_energy(spectrum, N)\n",
    "    amps = spectrum_smooth[peaks]\n",
    "    top_peaks = sorted([p for _, p in sorted(zip(amps, peaks), reverse=True)][:N-1])\n",
    "    boundaries = [p / len(spectrum) * np.pi for p in top_peaks]\n",
    "    return boundaries\n",
    "\n",
    "def make_filter_bank(boundaries, L):\n",
    "    freqs = np.linspace(0, np.pi, L//2 + 1)\n",
    "    mfb = []\n",
    "    # scaling lowpass\n",
    "    phi = np.zeros_like(freqs)\n",
    "    phi[freqs <= boundaries[0]] = 1\n",
    "    mfb.append(phi)\n",
    "    # wavelet bands\n",
    "    for i in range(len(boundaries)):\n",
    "        psi = np.zeros_like(freqs)\n",
    "        if i == len(boundaries) - 1:\n",
    "            mask = (freqs > boundaries[i])\n",
    "        else:\n",
    "            mask = (freqs > boundaries[i]) & (freqs <= boundaries[i+1])\n",
    "        psi[mask] = 1\n",
    "        mfb.append(psi)\n",
    "    return mfb\n",
    "\n",
    "def EWT1D(signal, N=3, smooth_sigma=2):\n",
    "    L = len(signal)\n",
    "    spectrum_half = np.abs(fft(signal))[:L//2 + 1]\n",
    "    boundaries = ewt_boundaries(spectrum_half, N, smooth_sigma=smooth_sigma)\n",
    "    mfb = make_filter_bank(boundaries, L)\n",
    "    modes = []\n",
    "    S_full = fft(signal)\n",
    "    for filt in mfb:\n",
    "        filt_full = np.concatenate([filt, filt[-2:0:-1]])\n",
    "        mode_freq = S_full * filt_full\n",
    "        mode_time = np.real(ifft(mode_freq))\n",
    "        modes.append(mode_time)\n",
    "    return np.array(modes), mfb, boundaries\n",
    "\n",
    "def iEWT1D(modes, mfb=None):\n",
    "    # simple inverse: sum modes\n",
    "    return np.sum(modes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6bc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivariate_lagged_dataset(df, target_col, feature_cols, lag=3):\n",
    "    data = df[feature_cols].values\n",
    "    target_idx = feature_cols.index(target_col)\n",
    "    X, y = [], []\n",
    "    for i in range(lag, len(df)):\n",
    "        X.append(data[i-lag:i].flatten())\n",
    "        y.append(data[i, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def safe_mape(y_true, y_pred, min_denom=1.0):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    mask = np.abs(y_true) >= min_denom\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100)\n",
    "\n",
    "def sde(y_true, y_pred):\n",
    "    return float(np.std(np.asarray(y_true) - np.asarray(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27054eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWO:\n",
    "    def __init__(self, obj_func, lb, ub, dim, n_agents=12, n_iter=25, seed=42):\n",
    "        self.obj_func = obj_func\n",
    "        self.lb = np.array(lb, dtype=float)\n",
    "        self.ub = np.array(ub, dtype=float)\n",
    "        self.dim = dim\n",
    "        self.n_agents = n_agents\n",
    "        self.n_iter = n_iter\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def optimize(self):\n",
    "        wolves = self.rng.uniform(self.lb, self.ub, size=(self.n_agents, self.dim))\n",
    "        fitness = np.array([self.obj_func(w) for w in wolves])\n",
    "        idx = np.argsort(fitness)\n",
    "        alpha, beta, delta = wolves[idx[0]].copy(), wolves[idx[1]].copy(), wolves[idx[2]].copy()\n",
    "        f_alpha, f_beta, f_delta = float(fitness[idx[0]]), float(fitness[idx[1]]), float(fitness[idx[2]])\n",
    "        for t in range(self.n_iter):\n",
    "            a = 2 - 2 * (t / (self.n_iter - 1 + 1e-9))\n",
    "            for i in range(self.n_agents):\n",
    "                X = wolves[i].copy()\n",
    "                for j in range(self.dim):\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A1 = 2 * a * r1 - a; C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * alpha[j] - X[j]); X1 = alpha[j] - A1 * D_alpha\n",
    "\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A2 = 2 * a * r1 - a; C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * beta[j] - X[j]); X2 = beta[j] - A2 * D_beta\n",
    "\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A3 = 2 * a * r1 - a; C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * delta[j] - X[j]); X3 = delta[j] - A3 * D_delta\n",
    "\n",
    "                    X[j] = (X1 + X2 + X3) / 3.0\n",
    "                wolves[i] = np.clip(X, self.lb, self.ub)\n",
    "            fitness = np.array([self.obj_func(w) for w in wolves])\n",
    "            idx = np.argsort(fitness)\n",
    "            if fitness[idx[0]] < f_alpha:\n",
    "                alpha, f_alpha = wolves[idx[0]].copy(), float(fitness[idx[0]])\n",
    "            if fitness[idx[1]] < f_beta:\n",
    "                beta, f_beta = wolves[idx[1]].copy(), float(fitness[idx[1]])\n",
    "            if fitness[idx[2]] < f_delta:\n",
    "                delta, f_delta = wolves[idx[2]].copy(), float(fitness[idx[2]])\n",
    "        return alpha, f_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c64a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_svr_params(position, logC_min=-3, logC_max=3, logg_min=-6, logg_max=0, eps_min=1e-4, eps_max=1.0):\n",
    "    log10C = float(np.clip(position[0], logC_min, logC_max))\n",
    "    log10gamma = float(np.clip(position[1], logg_min, logg_max))\n",
    "    eps_raw = float(position[2])\n",
    "    C = 10.0 ** log10C\n",
    "    gamma = 10.0 ** log10gamma\n",
    "    # map eps_raw in [0,1] to [eps_min, eps_max]\n",
    "    eps = eps_min + (eps_max - eps_min) * (np.clip(eps_raw, 0.0, 1.0))\n",
    "    return C, gamma, eps\n",
    "\n",
    "def make_svr_objective(X_train, y_train, X_val, y_val, random_state=42,\n",
    "                       logC_min=-3, logC_max=3, logg_min=-6, logg_max=0, eps_min=1e-4, eps_max=1.0):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    def objective(position):\n",
    "        C, gamma, eps = decode_svr_params(position, logC_min, logC_max, logg_min, logg_max, eps_min, eps_max)\n",
    "        try:\n",
    "            model = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=eps)\n",
    "            model.fit(X_train_s, y_train)\n",
    "            y_pred = model.predict(X_val_s)\n",
    "            return float(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        except Exception:\n",
    "            return 1e6\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456c7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewt_gwo_svm_iewt_pipeline(\n",
    "    df,\n",
    "    target_column,\n",
    "    feature_columns,\n",
    "    lag_steps=12,\n",
    "    n_modes=4,\n",
    "    gwo_agents=12,\n",
    "    gwo_iters=25,\n",
    "    random_state=42,\n",
    "    logC_min=-3, logC_max=3,\n",
    "    logg_min=-6, logg_max=0,\n",
    "    eps_min=1e-4, eps_max=1.0,\n",
    "    smooth_sigma=2,\n",
    "    max_step_eval=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dict with per-mode best params, one-step reconstructed metrics,\n",
    "      multi-step reconstructed metrics, boundaries, and predictions.\n",
    "    \"\"\"\n",
    "    # 1) EWT decomposition on full target\n",
    "    signal = df[target_column].values\n",
    "    modes, mfb, boundaries = EWT1D(signal, N=n_modes, smooth_sigma=smooth_sigma)\n",
    "\n",
    "    # 2) time-aware splits after creating lagged X (same across modes)\n",
    "    n_samples = len(signal) - lag_steps\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(\"lag_steps too large relative to series length.\")\n",
    "    train_end = int(0.7 * n_samples)\n",
    "    val_end = int(0.85 * n_samples)\n",
    "\n",
    "    per_mode_info = []\n",
    "    test_mode_preds = []\n",
    "    test_mode_lengths = []\n",
    "\n",
    "    # Process each mode independently\n",
    "    for mode_idx in range(n_modes):\n",
    "        mode_series = modes[mode_idx]\n",
    "        df_mode = df.copy()\n",
    "        df_mode[target_column] = mode_series\n",
    "\n",
    "        X_mode, y_mode = create_multivariate_lagged_dataset(df_mode, target_column, feature_columns, lag=lag_steps)\n",
    "        X_train, y_train = X_mode[:train_end], y_mode[:train_end]\n",
    "        X_val, y_val     = X_mode[train_end:val_end], y_mode[train_end:val_end]\n",
    "        X_test, y_test   = X_mode[val_end:], y_mode[val_end:]\n",
    "\n",
    "        # check sizes\n",
    "        if len(X_train) < 5 or len(X_val) < 1 or len(X_test) < 1:\n",
    "            print(f\"[Mode {mode_idx+1}] insufficient data â€“ skipping mode.\")\n",
    "            per_mode_info.append({\"mode\": mode_idx+1, \"skipped\": True})\n",
    "            test_mode_preds.append(np.zeros_like(X_test[:,0]) if X_test.size else np.array([]))\n",
    "            test_mode_lengths.append(len(X_test))\n",
    "            continue\n",
    "\n",
    "        # 3) Build objective for GWO\n",
    "        obj = make_svr_objective(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            random_state=random_state,\n",
    "            logC_min=logC_min, logC_max=logC_max,\n",
    "            logg_min=logg_min, logg_max=logg_max,\n",
    "            eps_min=eps_min, eps_max=eps_max\n",
    "        )\n",
    "\n",
    "        # GWO bounds: [log10C, log10gamma, eps_raw (0..1)]\n",
    "        lb = np.array([logC_min, logg_min, 0.0], dtype=float)\n",
    "        ub = np.array([logC_max, logg_max, 1.0], dtype=float)\n",
    "\n",
    "        gwo = GWO(obj, lb, ub, dim=3, n_agents=gwo_agents, n_iter=gwo_iters, seed=random_state + mode_idx)\n",
    "        best_pos, best_fit = gwo.optimize()\n",
    "\n",
    "        # decode params\n",
    "        C_best, gamma_best, eps_best = decode_svr_params(best_pos, logC_min, logC_max, logg_min, logg_max, eps_min, eps_max)\n",
    "\n",
    "        # 4) Train final SVR on train+val\n",
    "        scaler = StandardScaler()\n",
    "        X_trval = np.vstack([X_train, X_val])\n",
    "        y_trval = np.concatenate([y_train, y_val])\n",
    "        X_trval_s = scaler.fit_transform(X_trval)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        final_model = SVR(kernel='rbf', C=C_best, gamma=gamma_best, epsilon=eps_best)\n",
    "        final_model.fit(X_trval_s, y_trval)\n",
    "        y_pred_test_mode = final_model.predict(X_test_s)\n",
    "\n",
    "        per_mode_info.append({\n",
    "            \"mode\": mode_idx+1,\n",
    "            \"best_val_rmse\": float(best_fit),\n",
    "            \"log10C\": float(np.clip(best_pos[0], logC_min, logC_max)),\n",
    "            \"C\": float(C_best),\n",
    "            \"log10gamma\": float(np.clip(best_pos[1], logg_min, logg_max)),\n",
    "            \"gamma\": float(gamma_best),\n",
    "            \"epsilon\": float(eps_best),\n",
    "            \"test_len\": int(len(y_test))\n",
    "        })\n",
    "\n",
    "        test_mode_preds.append(y_pred_test_mode)\n",
    "        test_mode_lengths.append(len(y_test))\n",
    "\n",
    "    # 5) Align test predictions and IEWT reconstruct\n",
    "    n_test = None\n",
    "    for ln in test_mode_lengths:\n",
    "        if ln:\n",
    "            n_test = ln\n",
    "            break\n",
    "    if n_test is None:\n",
    "        raise RuntimeError(\"No valid test data across modes.\")\n",
    "\n",
    "    stacked = []\n",
    "    for arr in test_mode_preds:\n",
    "        a = np.asarray(arr)\n",
    "        if len(a) == n_test:\n",
    "            stacked.append(a)\n",
    "        elif len(a) == 0:\n",
    "            stacked.append(np.zeros(n_test))\n",
    "        elif len(a) < n_test:\n",
    "            stacked.append(np.concatenate([a, np.zeros(n_test - len(a))]))\n",
    "        else:\n",
    "            stacked.append(a[:n_test])\n",
    "    stacked = np.array(stacked)  # shape (n_modes, n_test)\n",
    "\n",
    "    y_pred_reconstructed = iEWT1D(stacked, mfb)\n",
    "    y_true_test = df[target_column].values[lag_steps + val_end : lag_steps + val_end + n_test]\n",
    "\n",
    "    # 6) One-step reconstructed metrics\n",
    "    one_mae = mean_absolute_error(y_true_test, y_pred_reconstructed)\n",
    "    one_rmse = np.sqrt(mean_squared_error(y_true_test, y_pred_reconstructed))\n",
    "    one_mape = safe_mape(y_true_test, y_pred_reconstructed)\n",
    "    one_sde = sde(y_true_test, y_pred_reconstructed)\n",
    "\n",
    "    one_step_metrics = {\"MAE\": float(one_mae), \"RMSE\": float(one_rmse), \"MAPE (%)\": float(one_mape) if not np.isnan(one_mape) else np.nan, \"SDE\": float(one_sde)}\n",
    "\n",
    "    # 7) Multi-step direct reconstructed metrics\n",
    "    multistep_rows = []\n",
    "    for step in range(1, max_step_eval + 1):\n",
    "        mode_step_preds = []\n",
    "        valid = True\n",
    "        for mode_idx in range(n_modes):\n",
    "            info = per_mode_info[mode_idx]\n",
    "            if info.get(\"skipped\", False):\n",
    "                mode_step_preds.append(np.zeros(max(0, n_test - step)))\n",
    "                continue\n",
    "\n",
    "            # recreate X_mode and splits\n",
    "            mode_series = modes[mode_idx]\n",
    "            df_mode = df.copy()\n",
    "            df_mode[target_column] = mode_series\n",
    "            X_mode, y_mode = create_multivariate_lagged_dataset(df_mode, target_column, feature_columns, lag=lag_steps)\n",
    "            X_train, y_train = X_mode[:train_end], y_mode[:train_end]\n",
    "            X_test_all, y_test_all = X_mode[val_end:], y_mode[val_end:]\n",
    "\n",
    "            if X_test_all.shape[0] <= step:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "            X_test_step = X_test_all[:-step]   # inputs to predict t+step\n",
    "            # train model on TRAIN ONLY with tuned params\n",
    "            C_best = info.get(\"C\", 1.0)\n",
    "            gamma_best = info.get(\"gamma\", 1.0)\n",
    "            eps_best = info.get(\"epsilon\", 0.1)\n",
    "\n",
    "            scaler_train = StandardScaler()\n",
    "            X_train_s = scaler_train.fit_transform(X_train)\n",
    "            X_test_step_s = scaler_train.transform(X_test_step)\n",
    "\n",
    "            model_step = SVR(kernel='rbf', C=C_best, gamma=gamma_best, epsilon=eps_best)\n",
    "            model_step.fit(X_train_s, y_train)\n",
    "            y_pred_step_mode = model_step.predict(X_test_step_s)  # length = n_test - step\n",
    "            mode_step_preds.append(y_pred_step_mode)\n",
    "\n",
    "        if not valid:\n",
    "            break\n",
    "\n",
    "        mode_step_preds = np.array(mode_step_preds)  # shape (n_modes, n_test-step)\n",
    "        y_pred_step_recon = iEWT1D(mode_step_preds, mfb=None)\n",
    "        y_true_step = df[target_column].values[lag_steps + val_end + step : lag_steps + val_end + step + y_pred_step_recon.shape[0]]\n",
    "\n",
    "        # align lengths\n",
    "        L_true = len(y_true_step); L_pred = len(y_pred_step_recon)\n",
    "        m = min(L_true, L_pred)\n",
    "        if m == 0:\n",
    "            break\n",
    "        y_true_step = y_true_step[:m]\n",
    "        y_pred_step_recon = y_pred_step_recon[:m]\n",
    "\n",
    "        multistep_rows.append({\n",
    "            \"Step\": step,\n",
    "            \"MAE\": float(mean_absolute_error(y_true_step, y_pred_step_recon)),\n",
    "            \"RMSE\": float(np.sqrt(mean_squared_error(y_true_step, y_pred_step_recon))),\n",
    "            \"MAPE (%)\": float(safe_mape(y_true_step, y_pred_step_recon)),\n",
    "            \"SDE\": float(sde(y_true_step, y_pred_step_recon))\n",
    "        })\n",
    "\n",
    "    multistep_df = pd.DataFrame(multistep_rows)\n",
    "\n",
    "    return {\n",
    "        \"per_mode_info\": per_mode_info,\n",
    "        \"one_step_metrics\": one_step_metrics,\n",
    "        \"multistep_df\": multistep_df,\n",
    "        \"boundaries_rad\": boundaries,\n",
    "        \"y_true_test\": y_true_test,\n",
    "        \"y_pred_reconstructed\": y_pred_reconstructed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dfe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['AirTemp','Azimuth','CloudOpacity','DewpointTemp','Dhi','Dni','Ebh',\n",
    "                   'WindDirection10m','Ghi','RelativeHumidity','SurfacePressure','WindSpeed10m']\n",
    "df = pd.read_csv('/Users/hrishityelchuri/Documents/windPred/raw/8.52 hrishit data.csv')\n",
    "\n",
    "df['PeriodEnd'] = pd.to_datetime(df['PeriodEnd'])\n",
    "df['PeriodStart'] = pd.to_datetime(df['PeriodStart'])\n",
    "df = df.sort_values('PeriodEnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ewt_gwo_svm_iewt_pipeline(\n",
    "    df,\n",
    "    target_column='WindSpeed10m',\n",
    "    feature_columns=feature_columns,\n",
    "    lag_steps=12,\n",
    "    n_modes=4,\n",
    "    gwo_agents=12,\n",
    "    gwo_iters=25,\n",
    "    random_state=42,\n",
    "    logC_min=-3, logC_max=3,\n",
    "    logg_min=-6, logg_max=0,\n",
    "    eps_min=1e-4, eps_max=0.5,\n",
    "    smooth_sigma=2,\n",
    "    max_step_eval=7\n",
    ")\n",
    "print(\"Per-mode best params:\")\n",
    "print(pd.DataFrame(res['per_mode_info']))\n",
    "print(\"\\nOne-step reconstructed metrics:\")\n",
    "print(res['one_step_metrics'])\n",
    "print(\"\\nMulti-step reconstructed metrics:\")\n",
    "print(res['multistep_df'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c851b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
