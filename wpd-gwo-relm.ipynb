{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90149e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b3cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RELM:\n",
    "    def __init__(self, n_hidden=100, activation='tanh', C=1.0, random_state=None):\n",
    "        self.n_hidden = int(n_hidden)\n",
    "        self.activation = activation\n",
    "        self.C = float(C)\n",
    "        self.random_state = random_state\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _init_weights(self, n_features):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.W = rng.uniform(-1, 1, size=(self.n_hidden, n_features))\n",
    "        self.b = rng.uniform(-1, 1, size=(self.n_hidden,))\n",
    "\n",
    "    def _activation(self, X):\n",
    "        if self.activation == 'sigmoid':\n",
    "            X = np.clip(X, -500, 500)\n",
    "            return 1.0 / (1.0 + np.exp(-X))\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(X)\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0.0, X)\n",
    "        raise ValueError(\"Unknown activation\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X); y = np.asarray(y)\n",
    "        if y.ndim == 1: y = y.reshape(-1, 1)\n",
    "        N, d = X.shape\n",
    "        self._init_weights(d)\n",
    "        H = self._activation(X @ self.W.T + self.b)\n",
    "        # Ridge-regularized least squares (two cases for stability)\n",
    "        if N >= self.n_hidden:\n",
    "            A = (np.eye(self.n_hidden) / self.C) + (H.T @ H)\n",
    "            B = H.T @ y\n",
    "            self.beta = np.linalg.solve(A, B)\n",
    "        else:\n",
    "            A = (np.eye(N) / self.C) + (H @ H.T)\n",
    "            B = y\n",
    "            self.beta = H.T @ np.linalg.solve(A, B)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation(np.asarray(X) @ self.W.T + self.b)\n",
    "        Y = H @ self.beta\n",
    "        return Y.ravel() if Y.shape[1] == 1 else Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17333f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWO:\n",
    "    def __init__(self, obj_func, lb, ub, dim, n_agents=12, n_iter=25, seed=42):\n",
    "        self.obj = obj_func\n",
    "        self.lb = np.array(lb, dtype=float)\n",
    "        self.ub = np.array(ub, dtype=float)\n",
    "        self.dim = dim\n",
    "        self.n_agents = n_agents\n",
    "        self.n_iter = n_iter\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def optimize(self):\n",
    "        wolves = self.rng.uniform(self.lb, self.ub, size=(self.n_agents, self.dim))\n",
    "        fitness = np.array([self.obj(w) for w in wolves])\n",
    "        idx = np.argsort(fitness)\n",
    "        alpha, beta, delta = wolves[idx[0]].copy(), wolves[idx[1]].copy(), wolves[idx[2]].copy()\n",
    "        f_alpha, f_beta, f_delta = float(fitness[idx[0]]), float(fitness[idx[1]]), float(fitness[idx[2]])\n",
    "        for t in range(self.n_iter):\n",
    "            a = 2 - 2 * (t / (self.n_iter - 1 + 1e-9))\n",
    "            for i in range(self.n_agents):\n",
    "                X = wolves[i].copy()\n",
    "                for j in range(self.dim):\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A1 = 2 * a * r1 - a; C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * alpha[j] - X[j]); X1 = alpha[j] - A1 * D_alpha\n",
    "\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A2 = 2 * a * r1 - a; C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * beta[j] - X[j]); X2 = beta[j] - A2 * D_beta\n",
    "\n",
    "                    r1, r2 = self.rng.random(), self.rng.random()\n",
    "                    A3 = 2 * a * r1 - a; C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * delta[j] - X[j]); X3 = delta[j] - A3 * D_delta\n",
    "\n",
    "                    X[j] = (X1 + X2 + X3) / 3.0\n",
    "                wolves[i] = np.clip(X, self.lb, self.ub)\n",
    "            fitness = np.array([self.obj(w) for w in wolves])\n",
    "            idx = np.argsort(fitness)\n",
    "            if fitness[idx[0]] < f_alpha:\n",
    "                alpha, f_alpha = wolves[idx[0]].copy(), float(fitness[idx[0]])\n",
    "            if fitness[idx[1]] < f_beta:\n",
    "                beta, f_beta = wolves[idx[1]].copy(), float(fitness[idx[1]])\n",
    "            if fitness[idx[2]] < f_delta:\n",
    "                delta, f_delta = wolves[idx[2]].copy(), float(fitness[idx[2]])\n",
    "        return alpha, f_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbc4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# db4 analysis/synthesis filter taps\n",
    "# (orthonormal Daubechies-4; length 8)\n",
    "# =====================================\n",
    "def db4_filters():\n",
    "    # Decomposition low-pass (h0) and high-pass (h1)\n",
    "    h0 = np.array([\n",
    "        -0.010597401784997278,\n",
    "         0.032883011666982945,\n",
    "         0.030841381835560763,\n",
    "        -0.18703481171888114,\n",
    "        -0.027983769416859854,\n",
    "         0.6308807679298589,\n",
    "         0.7148465705525415,\n",
    "         0.23037781330885523\n",
    "    ], dtype=float)\n",
    "    # High-pass from low-pass with quadrature mirror property\n",
    "    h1 = np.array([(-1)**k * h0[::-1][k] for k in range(len(h0))], dtype=float)\n",
    "\n",
    "    # Reconstruction filters (g0, g1) for orthonormal wavelets\n",
    "    g0 = h0[::-1]  # synthesis low-pass\n",
    "    g1 = -h1[::-1] # synthesis high-pass\n",
    "    return h0, h1, g0, g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bb54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_reflect_same(x, h):\n",
    "    \"\"\"\n",
    "    1D convolution with 'symmetric' / reflect padding so that output length == len(x).\n",
    "    \"\"\"\n",
    "    m = len(h)\n",
    "    pad = m - 1\n",
    "    xpad = np.pad(x, (pad, pad), mode='reflect')\n",
    "    y = signal.convolve(xpad, h, mode='valid')  # valid yields len(xpad) - m + 1 = len(x)+pad\n",
    "    # Correct length to exactly len(x)\n",
    "    if len(y) > len(x):\n",
    "        # Center crop\n",
    "        start = (len(y) - len(x)) // 2\n",
    "        y = y[start:start+len(x)]\n",
    "    elif len(y) < len(x):\n",
    "        y = np.pad(y, (0, len(x)-len(y)), mode='constant')\n",
    "    return y\n",
    "\n",
    "def downsample2(x, phase=0):\n",
    "    return x[phase::2]\n",
    "\n",
    "def upsample2(x):\n",
    "    y = np.zeros(2 * len(x), dtype=float)\n",
    "    y[::2] = x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05b728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wpd_db4_decompose(signal_in, level=3):\n",
    "    \"\"\"\n",
    "    Build full WPD binary tree to `level` using db4 filters.\n",
    "    Returns:\n",
    "      leaves: dict path -> coeff array  (paths are strings of 'a'/'d', length==level)\n",
    "      all_nodes: dict path -> coeff array  (includes intermediate nodes)\n",
    "    \"\"\"\n",
    "    h0, h1, _, _ = db4_filters()\n",
    "    x0 = np.asarray(signal_in, dtype=float)\n",
    "    nodes = {'': x0}  # root path ''\n",
    "    # For each level, split every current node into 'a' and 'd'\n",
    "    for L in range(1, level+1):\n",
    "        new_nodes = {}\n",
    "        for path, x in nodes.items():\n",
    "            # Analysis filtering + downsampling\n",
    "            approx = downsample2(conv_reflect_same(x, h0), phase=0)\n",
    "            detail = downsample2(conv_reflect_same(x, h1), phase=0)\n",
    "            new_nodes[path + 'a'] = approx\n",
    "            new_nodes[path + 'd'] = detail\n",
    "        nodes = new_nodes\n",
    "    # At the end, nodes are just the leaves at depth `level`\n",
    "    leaves = nodes\n",
    "    # Build dict of all nodes for potential debugging (optional)\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db15913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_leaf_db4(leaf_coeffs, path):\n",
    "    \"\"\"\n",
    "    Reconstruct time-domain contribution of a single leaf given its coefficients and path.\n",
    "    Path is a string like 'aad' meaning at each synthesis step use:\n",
    "      - g0 if char=='a' (low branch), g1 if 'd' (high branch)\n",
    "    \"\"\"\n",
    "    _, _, g0, g1 = db4_filters()\n",
    "    y = np.asarray(leaf_coeffs, dtype=float)\n",
    "    # Walk from leaf back to root (reverse path)\n",
    "    for ch in reversed(path):\n",
    "        y_up = upsample2(y)\n",
    "        if ch == 'a':\n",
    "            y = conv_reflect_same(y_up, g0)\n",
    "        else:\n",
    "            y = conv_reflect_same(y_up, g1)\n",
    "    return y  # length should be close to original (boundary effects handled by reflect)\n",
    "\n",
    "def wpd_db4_reconstruct_components(signal_in, level=3):\n",
    "    \"\"\"\n",
    "    Decompose and then reconstruct each leaf component to original length.\n",
    "    Returns:\n",
    "      comp_list: list of arrays (each approx original length)\n",
    "      labels: list of leaf paths in order\n",
    "    \"\"\"\n",
    "    leaves = wpd_db4_decompose(signal_in, level=level)\n",
    "    labels = sorted(leaves.keys())  # deterministic order (lexicographic)\n",
    "    comps = []\n",
    "    N = len(signal_in)\n",
    "    for p in labels:\n",
    "        c = leaves[p]\n",
    "        rec = reconstruct_leaf_db4(c, p)\n",
    "        # Adjust length with center crop/pad to exactly N\n",
    "        if len(rec) > N:\n",
    "            start = (len(rec) - N) // 2\n",
    "            rec = rec[start:start+N]\n",
    "        elif len(rec) < N:\n",
    "            rec = np.pad(rec, (0, N-len(rec)), mode='constant')\n",
    "        comps.append(rec)\n",
    "    return comps, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b59c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivariate_lagged_dataset(df, target_col, feature_cols, lag=12):\n",
    "    data = df[feature_cols].values\n",
    "    target_idx = feature_cols.index(target_col)\n",
    "    X, y = [], []\n",
    "    for i in range(lag, len(df)):\n",
    "        X.append(data[i-lag:i].flatten())\n",
    "        y.append(data[i, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def safe_mape(y_true, y_pred, min_denom=1.0):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    mask = np.abs(y_true) >= min_denom\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100)\n",
    "\n",
    "def sde(y_true, y_pred):\n",
    "    return float(np.std(np.asarray(y_true) - np.asarray(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a14597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_relm_params(position, Hmin=20, Hmax=500, Cmin_log=-4, Cmax_log=4):\n",
    "    h_raw, c_log, a_raw = position\n",
    "    n_hidden = int(np.round(Hmin + np.clip(h_raw, 0, 1) * (Hmax - Hmin)))\n",
    "    n_hidden = int(np.clip(n_hidden, Hmin, Hmax))\n",
    "    C = 10.0 ** float(np.clip(c_log, Cmin_log, Cmax_log))\n",
    "    act_idx = int(np.round(np.clip(a_raw, 0, 2)))\n",
    "    activation = ['tanh', 'sigmoid', 'relu'][act_idx]\n",
    "    return n_hidden, C, activation\n",
    "\n",
    "def make_relm_objective(X_train, y_train, X_val, y_val, random_state=42,\n",
    "                        Hmin=20, Hmax=500, Cmin_log=-4, Cmax_log=4):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    def objective(position):\n",
    "        n_hidden, C, activation = decode_relm_params(position, Hmin=Hmin, Hmax=Hmax, Cmin_log=Cmin_log, Cmax_log=Cmax_log)\n",
    "        try:\n",
    "            mdl = RELM(n_hidden=n_hidden, activation=activation, C=C, random_state=random_state)\n",
    "            mdl.fit(X_train_s, y_train)\n",
    "            y_p = mdl.predict(X_val_s)\n",
    "            return float(np.sqrt(mean_squared_error(y_val, y_p)))\n",
    "        except Exception:\n",
    "            return 1e6\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb4c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wpd_gwo_relm_pipeline_scipy(\n",
    "    df,\n",
    "    target_column,\n",
    "    feature_columns,\n",
    "    lag_steps=12,\n",
    "    level=3,\n",
    "    gwo_agents=12,\n",
    "    gwo_iters=25,\n",
    "    random_state=42,\n",
    "    Hmin=20, Hmax=400,\n",
    "    Cmin_log=-4, Cmax_log=4,\n",
    "    max_step_eval=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - per_component_info: tuned params per WPD leaf (path label)\n",
    "      - one_step_metrics: dict\n",
    "      - multistep_df: DataFrame with Step, MAE, RMSE, MAPE, SDE\n",
    "      - components: list of time-domain leaf components\n",
    "      - labels: list of leaf paths (e.g., 'aad', 'ada', ...)\n",
    "      - y_true_test, y_pred_reconstructed\n",
    "    \"\"\"\n",
    "    y = df[target_column].values.astype(float)\n",
    "    # 1) WPD(db4) components\n",
    "    components, labels = wpd_db4_reconstruct_components(y, level=level)\n",
    "    n_comp = len(components)\n",
    "    if n_comp == 0:\n",
    "        raise RuntimeError(\"No WPD components produced.\")\n",
    "\n",
    "    # 2) lagged split indices (shared across components)\n",
    "    N = len(y)\n",
    "    n_samples = N - lag_steps\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(\"lag_steps too large for series length.\")\n",
    "    train_end = int(0.7 * n_samples)\n",
    "    val_end = int(0.85 * n_samples)\n",
    "\n",
    "    per_component_info = []\n",
    "    test_comp_preds = []\n",
    "    test_comp_lengths = []\n",
    "\n",
    "    # 3) Per-leaf model\n",
    "    for ci, comp in enumerate(components):\n",
    "        df_c = df.copy()\n",
    "        df_c[target_column] = comp  # target is this leaf component\n",
    "\n",
    "        Xc, yc = create_multivariate_lagged_dataset(df_c, target_column, feature_columns, lag=lag_steps)\n",
    "        X_train, y_train = Xc[:train_end], yc[:train_end]\n",
    "        X_val, y_val     = Xc[train_end:val_end], yc[train_end:val_end]\n",
    "        X_test, y_test   = Xc[val_end:], yc[val_end:]\n",
    "\n",
    "        if len(X_train) < 5 or len(X_val) < 1 or len(X_test) < 1:\n",
    "            per_component_info.append({\"component\": ci+1, \"label\": labels[ci], \"skipped\": True})\n",
    "            test_comp_preds.append(np.zeros_like(X_test[:,0]) if X_test.size else np.array([]))\n",
    "            test_comp_lengths.append(len(X_test))\n",
    "            continue\n",
    "\n",
    "        # GWO tuning\n",
    "        obj = make_relm_objective(X_train, y_train, X_val, y_val, random_state=random_state,\n",
    "                                  Hmin=Hmin, Hmax=Hmax, Cmin_log=Cmin_log, Cmax_log=Cmax_log)\n",
    "        lb = np.array([0.0, Cmin_log, 0.0], dtype=float)\n",
    "        ub = np.array([1.0, Cmax_log, 2.0], dtype=float)\n",
    "        gwo = GWO(obj, lb, ub, dim=3, n_agents=gwo_agents, n_iter=gwo_iters, seed=random_state + ci)\n",
    "        best_pos, best_fit = gwo.optimize()\n",
    "        n_hidden, C, activation = decode_relm_params(best_pos, Hmin=Hmin, Hmax=Hmax, Cmin_log=Cmin_log, Cmax_log=Cmax_log)\n",
    "\n",
    "        # Final train on train+val\n",
    "        scaler = StandardScaler()\n",
    "        X_trval = np.vstack([X_train, X_val])\n",
    "        y_trval = np.concatenate([y_train, y_val])\n",
    "        X_trval_s = scaler.fit_transform(X_trval)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        mdl = RELM(n_hidden=n_hidden, activation=activation, C=C, random_state=random_state)\n",
    "        mdl.fit(X_trval_s, y_trval)\n",
    "        y_pred_test_comp = mdl.predict(X_test_s)\n",
    "\n",
    "        per_component_info.append({\n",
    "            \"component\": ci+1,\n",
    "            \"label\": labels[ci],\n",
    "            \"best_val_rmse\": float(best_fit),\n",
    "            \"n_hidden\": int(n_hidden),\n",
    "            \"C\": float(C),\n",
    "            \"activation\": activation,\n",
    "            \"test_len\": int(len(y_test))\n",
    "        })\n",
    "        test_comp_preds.append(y_pred_test_comp)\n",
    "        test_comp_lengths.append(len(y_test))\n",
    "\n",
    "    # 4) Reconstruct final test forecast by summation\n",
    "    # Find common n_test (first non-zero length)\n",
    "    n_test = next((ln for ln in test_comp_lengths if ln), None)\n",
    "    if n_test is None:\n",
    "        raise RuntimeError(\"No valid test length across components.\")\n",
    "    stacked = []\n",
    "    for arr in test_comp_preds:\n",
    "        a = np.asarray(arr)\n",
    "        if len(a) == n_test:\n",
    "            stacked.append(a)\n",
    "        elif len(a) == 0:\n",
    "            stacked.append(np.zeros(n_test))\n",
    "        elif len(a) < n_test:\n",
    "            stacked.append(np.concatenate([a, np.zeros(n_test - len(a))]))\n",
    "        else:\n",
    "            stacked.append(a[:n_test])\n",
    "    stacked = np.array(stacked)\n",
    "    y_pred_reconstructed = np.sum(stacked, axis=0)\n",
    "    y_true_test = df[target_column].values[lag_steps + val_end : lag_steps + val_end + n_test]\n",
    "\n",
    "    # 5) One-step metrics\n",
    "    one_mae  = mean_absolute_error(y_true_test, y_pred_reconstructed)\n",
    "    one_rmse = np.sqrt(mean_squared_error(y_true_test, y_pred_reconstructed))\n",
    "    one_mape = safe_mape(y_true_test, y_pred_reconstructed)\n",
    "    one_sdev = sde(y_true_test, y_pred_reconstructed)\n",
    "    one_step_metrics = {\"MAE\": float(one_mae), \"RMSE\": float(one_rmse),\n",
    "                        \"MAPE (%)\": float(one_mape) if not np.isnan(one_mape) else np.nan,\n",
    "                        \"SDE\": float(one_sdev)}\n",
    "\n",
    "    # 6) Multi-step (direct) metrics (1..H) using TRAIN only with tuned params\n",
    "    multistep_rows = []\n",
    "    for step in range(1, max_step_eval + 1):\n",
    "        comp_step_preds = []\n",
    "        valid = True\n",
    "        for info in per_component_info:\n",
    "            if info.get(\"skipped\", False):\n",
    "                comp_step_preds.append(np.zeros(max(0, n_test - step)))\n",
    "                continue\n",
    "            ci = info[\"component\"] - 1\n",
    "            comp_series = components[ci]\n",
    "            df_c = df.copy(); df_c[target_column] = comp_series\n",
    "            Xc, yc = create_multivariate_lagged_dataset(df_c, target_column, feature_columns, lag=lag_steps)\n",
    "            X_train, y_train = Xc[:train_end], yc[:train_end]\n",
    "            X_test_all, y_test_all = Xc[val_end:], yc[val_end:]\n",
    "            if X_test_all.shape[0] <= step:\n",
    "                valid = False; break\n",
    "            X_test_step = X_test_all[:-step]\n",
    "\n",
    "            n_hidden = info[\"n_hidden\"]; C = info[\"C\"]; activation = info[\"activation\"]\n",
    "            scaler_tr = StandardScaler()\n",
    "            X_train_s = scaler_tr.fit_transform(X_train)\n",
    "            X_test_step_s = scaler_tr.transform(X_test_step)\n",
    "            mdl = RELM(n_hidden=n_hidden, activation=activation, C=C, random_state=random_state)\n",
    "            mdl.fit(X_train_s, y_train)\n",
    "            y_pred_step_comp = mdl.predict(X_test_step_s)\n",
    "            comp_step_preds.append(y_pred_step_comp)\n",
    "\n",
    "        if not valid:\n",
    "            break\n",
    "\n",
    "        comp_step_preds = np.array(comp_step_preds)\n",
    "        y_pred_step_recon = np.sum(comp_step_preds, axis=0)\n",
    "        y_true_step = df[target_column].values[lag_steps + val_end + step :\n",
    "                                               lag_steps + val_end + step + y_pred_step_recon.shape[0]]\n",
    "        m = min(len(y_true_step), len(y_pred_step_recon))\n",
    "        if m == 0:\n",
    "            break\n",
    "        y_true_step = y_true_step[:m]; y_pred_step_recon = y_pred_step_recon[:m]\n",
    "        multistep_rows.append({\n",
    "            \"Step\": step,\n",
    "            \"MAE\": float(mean_absolute_error(y_true_step, y_pred_step_recon)),\n",
    "            \"RMSE\": float(np.sqrt(mean_squared_error(y_true_step, y_pred_step_recon))),\n",
    "            \"MAPE (%)\": float(safe_mape(y_true_step, y_pred_step_recon)),\n",
    "            \"SDE\": float(sde(y_true_step, y_pred_step_recon))\n",
    "        })\n",
    "\n",
    "    multistep_df = pd.DataFrame(multistep_rows)\n",
    "\n",
    "    return {\n",
    "        \"per_component_info\": per_component_info,\n",
    "        \"one_step_metrics\": one_step_metrics,\n",
    "        \"multistep_df\": multistep_df,\n",
    "        \"components\": components,\n",
    "        \"labels\": labels,\n",
    "        \"y_true_test\": y_true_test,\n",
    "        \"y_pred_reconstructed\": y_pred_reconstructed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['AirTemp','Azimuth','CloudOpacity','DewpointTemp','Dhi','Dni','Ebh',\n",
    "                   'WindDirection10m','Ghi','RelativeHumidity','SurfacePressure','WindSpeed10m']\n",
    "df = pd.read_csv('/Users/hrishityelchuri/Documents/windPred/raw/8.52 hrishit data.csv')\n",
    "\n",
    "df['PeriodEnd'] = pd.to_datetime(df['PeriodEnd'])\n",
    "df['PeriodStart'] = pd.to_datetime(df['PeriodStart'])\n",
    "df = df.sort_values('PeriodEnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c15ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-leaf tuned params:\n",
      "   component label  best_val_rmse  n_hidden             C activation  test_len\n",
      "0          1   aaa       0.419117       395   6406.834868       relu     20612\n",
      "1          2   aad       0.103625       386   2414.919644       relu     20612\n",
      "2          3   ada       0.024640       398      0.004908       relu     20612\n",
      "3          4   add       0.065155       394     11.393172       relu     20612\n",
      "4          5   daa       0.003505       398      0.028129       relu     20612\n",
      "5          6   dad       0.008660       396   2010.817517       relu     20612\n",
      "6          7   dda       0.016320       398    621.080933       relu     20612\n",
      "7          8   ddd       0.015858       399  10000.000000       relu     20612\n",
      "\n",
      "One-step metrics: {'MAE': 0.9793318821636319, 'RMSE': 1.208473003386571, 'MAPE (%)': 37.61190542872876, 'SDE': 1.2029115094272225}\n",
      "\n",
      "Multi-step metrics:\n",
      " Step      MAE     RMSE  MAPE (%)      SDE\n",
      "    1 1.206712 1.459650 47.083456 1.396769\n",
      "    2 1.286172 1.557432 49.392221 1.498638\n",
      "    3 1.338393 1.625478 50.823674 1.569209\n",
      "    4 1.357305 1.655083 51.492847 1.599831\n",
      "    5 1.349818 1.650108 51.539471 1.594656\n",
      "    6 1.326539 1.622497 51.274713 1.566031\n",
      "    7 1.294923 1.583708 50.639818 1.525760\n"
     ]
    }
   ],
   "source": [
    "res = wpd_gwo_relm_pipeline_scipy(\n",
    "    df,\n",
    "    target_column='WindSpeed10m',\n",
    "    feature_columns=feature_columns,\n",
    "    lag_steps=12,\n",
    "    level=3,              # depth of packet tree; try 2..5\n",
    "    gwo_agents=12,\n",
    "    gwo_iters=25,\n",
    "    random_state=42,\n",
    "    Hmin=20, Hmax=400,\n",
    "    Cmin_log=-4, Cmax_log=4,\n",
    "    max_step_eval=7\n",
    ")\n",
    "print(\"Per-leaf tuned params:\")\n",
    "print(pd.DataFrame(res['per_component_info']))\n",
    "print(\"\\nOne-step metrics:\", res['one_step_metrics'])\n",
    "print(\"\\nMulti-step metrics:\")\n",
    "print(res['multistep_df'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627823f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
